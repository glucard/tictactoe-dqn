{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from src.state_generator import generate_states\n",
    "\n",
    "if not os.path.exists(\"data/states.json\"):\n",
    "    generate_states()\n",
    "with open(\"data/states.json\") as file:\n",
    "    states_json = json.load(file)\n",
    "    states_dict = states_json['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action','next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    self.layer_input = nn.Linear(n_observations, 512)\n",
    "    self.layer_h_1 = nn.Linear(512, 512)\n",
    "    self.layer_h_2 = nn.Linear(512, 512)\n",
    "    self.layer_v = nn.Linear(512, 1)\n",
    "    self.layer_a = nn.Linear(512, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_input(x))\n",
    "    # x = self.dropout0(x)\n",
    "    x = F.relu(self.layer_h_1(x))\n",
    "    # x = self.dropout1(x)\n",
    "    x = F.relu(self.layer_h_2(x))\n",
    "    # x = self.dropout2(x)\n",
    "    \n",
    "    v = self.layer_v(x)\n",
    "    a = self.layer_a(x)\n",
    "    \n",
    "    q = v + a - a.mean()\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tictactoe import decode as state_decode\n",
    "\n",
    "encoded_states = [s['encoded'] for s in states_dict]\n",
    "decoded_states = [state_decode(es) for es in encoded_states]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_marks = {\n",
    "    'x': {\n",
    "        'x': 1,\n",
    "        'o': -1,\n",
    "        '-': 0,\n",
    "    },\n",
    "    'o': {\n",
    "        'x': -1,\n",
    "        'o': 1,\n",
    "        '-': 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_game_obs(state_dict: dict) -> list:\n",
    "    turn_mark = 'x' if state_dict['turn'] % 2 == 0 else 'o'\n",
    "    return [turn_marks[turn_mark][e] for e in state_dict['encoded']]\n",
    "\n",
    "all_games_obs = [get_game_obs(sd) for sd in states_dict]\n",
    "all_games_actions = [sd['actions'] for sd in states_dict]\n",
    "all_games_isdone = [sd['done'] for sd in states_dict]\n",
    "all_games_winner = [sd['winner'] for sd in states_dict]\n",
    "all_games_possible_wins = [sd['possible_wins'] for sd in states_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 9\n",
    "n_actions = 9\n",
    "\n",
    "REPLAY_SIZE = len(all_games_obs) * n_actions**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.109163928441037%\n",
      "18.236582694414018%\n",
      "27.364001460387%\n",
      "36.491420226359985%\n",
      "45.61883899233297%\n",
      "54.74625775830595%\n",
      "63.873676524278935%\n",
      "73.00109529025191%\n",
      "82.1285140562249%\n",
      "91.25593282219788%\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "n = len(states_dict)\n",
    "for sd_id, sd in enumerate(states_dict):\n",
    "    if sd_id % 500 == 499:\n",
    "        print(f\"{100 * sd_id/n}%\")\n",
    "    obs = torch.tensor(get_game_obs(sd), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    for i in range(n_actions):\n",
    "        \n",
    "        # if done, then cannot take an action. So, continue\n",
    "        if sd['done']:\n",
    "            continue\n",
    "\n",
    "        action = torch.tensor([[i]], dtype=torch.long).to(device)\n",
    "        oponent_sd = states_dict[sd['actions'][i]]\n",
    "        if oponent_sd['done']:\n",
    "            reward = 0\n",
    "            turn_mark = 'x' if sd['turn'] % 2 == 0 else 'o'\n",
    "            if sd['winner'] != '-':\n",
    "                # win or lose\n",
    "                reward += 1 if oponent_sd['winner'] == turn_mark else -1\n",
    "            # draw\n",
    "            reward += 0\n",
    "            \n",
    "            reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "            memory.push(obs, action, None, reward)\n",
    "        else:\n",
    "            for j in(range(n_actions)):\n",
    "                \n",
    "                # next_states for each oponent action based on the \"player\" action.\n",
    "                next_sd = states_dict[oponent_sd['actions'][j]]\n",
    "                next_obs = torch.tensor(get_game_obs(next_sd), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "                reward = 0\n",
    "                if sd == oponent_sd:\n",
    "                    # invalid move\n",
    "                    reward -= 2\n",
    "                else:\n",
    "                    reward += next_sd['possible_wins']\n",
    "\n",
    "\n",
    "                reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "\n",
    "                # ('state', 'action','next_state', 'reward')\n",
    "                memory.push(obs, action, next_obs, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343224"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 343224 // 50 #1024\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "LR = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR) # amsgrad? r:\n",
    "global_step = 0\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # print after\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch) # print after\n",
    "    state_action_values = state_action_values.gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    expected_next_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_next_action_values.unsqueeze(1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  max_epoch = 200\n",
    "else:\n",
    "  max_epoch = 50\n",
    "\n",
    "h_params = {\n",
    "    'REPLAY_SIZE': REPLAY_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'TAU': TAU,\n",
    "    'LR': LR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "ep_losses = []\n",
    "\n",
    "with SummaryWriter(log_dir=f'duel_runs/{timestr}') as writer:\n",
    "    \n",
    "    while global_step < max_epoch:\n",
    "\n",
    "\n",
    "        loss_scalar = optimize_model()\n",
    "        ep_losses.append(loss_scalar)\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "\n",
    "        loss_mean = np.mean(ep_losses)\n",
    "        \n",
    "        writer.add_hparams(\n",
    "            h_params,\n",
    "            {\n",
    "                'i_episode': global_step,\n",
    "                'Memory_len': len(memory),\n",
    "                'Loss': loss_mean, # loss_scalar,\n",
    "            }, name='.', global_step=global_step,\n",
    "        )\n",
    "        ep_losses = []\n",
    "        \n",
    "        global_step += 1 \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_model_dir = './duel_saved_models'\n",
    "\n",
    "if not os.path.exists(f'{save_model_dir}'):\n",
    "    os.mkdir(f'{save_model_dir}')\n",
    "if not os.path.exists(f'{save_model_dir}/{timestr}'):\n",
    "    os.mkdir(f'{save_model_dir}/{timestr}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), f'{save_model_dir}/{timestr}/policy_net')\n",
    "torch.save(target_net.state_dict(), f'{save_model_dir}/{timestr}/target_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\\n\\nmodel = DQN(n_observations, n_actions).to(device)\\nload_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\\nmodel.load_state_dict(load_dict)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "load_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\n",
    "model.load_state_dict(load_dict)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]\n",
      "tensor([[6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#_, ax = plt.subplots(1, 1)\n",
    "\n",
    "#img = ax.imshow(env.render())\n",
    "\n",
    "\n",
    "while True:\n",
    "  # policy_net.eval()\n",
    "  state = states_dict[0]\n",
    "\n",
    "  for t in count():\n",
    "    obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    print(obs)\n",
    "    for line in state_decode(state['encoded']):\n",
    "      print(line)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      action = policy_net(obs)\n",
    "      action = action.max(1)[1].view(1,1)\n",
    "      print(action)\n",
    "    \n",
    "    input()\n",
    "    next_state = states_dict[state['actions'][action.item()]]\n",
    "    done = next_state['done']\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "      print(obs)\n",
    "      input()\n",
    "      break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
