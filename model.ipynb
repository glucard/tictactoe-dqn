{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from src.state_generator import generate_states\n",
    "\n",
    "if not os.path.exists(\"data/states.json\"):\n",
    "    generate_states()\n",
    "with open(\"data/states.json\") as file:\n",
    "    states_json = json.load(file)\n",
    "    states_dict = states_json['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action','next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    self.layer_input = nn.Linear(n_observations, 2048)\n",
    "    self.layer_h_1 = nn.Linear(2048, 2048)\n",
    "    #self.layer_h_2 = nn.Linear(512, 512)\n",
    "    self.layer_v = nn.Linear(2048, 1)\n",
    "    self.layer_a = nn.Linear(2048, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_input(x))\n",
    "    # x = self.dropout0(x)\n",
    "    x = F.relu(self.layer_h_1(x))\n",
    "    # x = self.dropout1(x)\n",
    "    # x = F.relu(self.layer_h_2(x))\n",
    "    # x = self.dropout2(x)\n",
    "    \n",
    "    v = self.layer_v(x)\n",
    "    a = self.layer_a(x)\n",
    "    \n",
    "    q = v + a - a.mean()\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tictactoe import decode as state_decode\n",
    "\n",
    "encoded_states = [s['encoded'] for s in states_dict]\n",
    "decoded_states = [state_decode(es) for es in encoded_states]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_marks = {\n",
    "    'x': {\n",
    "        'x': 1,\n",
    "        'o': -1,\n",
    "        '-': 0,\n",
    "    },\n",
    "    'o': {\n",
    "        'x': -1,\n",
    "        'o': 1,\n",
    "        '-': 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_game_obs(state_dict: dict) -> list:\n",
    "    turn_mark = 'x' if state_dict['turn'] % 2 == 0 else 'o'\n",
    "    return [turn_marks[turn_mark][e] for e in state_dict['encoded']]\n",
    "\n",
    "all_games_obs = [get_game_obs(sd) for sd in states_dict]\n",
    "all_games_actions = [sd['actions'] for sd in states_dict]\n",
    "all_games_isdone = [sd['done'] for sd in states_dict]\n",
    "all_games_winner = [sd['winner'] for sd in states_dict]\n",
    "all_games_possible_wins = [sd['possible_wins'] for sd in states_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 9\n",
    "n_actions = 9\n",
    "\n",
    "REPLAY_SIZE = len(all_games_obs) * n_actions**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.109163928441037%\n",
      "18.236582694414018%\n",
      "27.364001460387%\n",
      "36.491420226359985%\n",
      "45.61883899233297%\n",
      "54.74625775830595%\n",
      "63.873676524278935%\n",
      "73.00109529025191%\n",
      "82.1285140562249%\n",
      "91.25593282219788%\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "n = len(states_dict)\n",
    "for sd_id, sd in enumerate(states_dict):\n",
    "    if sd_id % 500 == 499:\n",
    "        print(f\"{100 * sd_id/n}%\")\n",
    "    obs = torch.tensor(get_game_obs(sd), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "    for i in range(n_actions):\n",
    "        \n",
    "        # if done, then cannot take an action. So, continue\n",
    "        if sd['done']:\n",
    "            continue\n",
    "\n",
    "        action = torch.tensor([[i]], dtype=torch.long).to(device)\n",
    "        oponent_sd = states_dict[sd['actions'][i]]\n",
    "        turn_mark = 'x' if sd['turn'] % 2 == 0 else 'o'\n",
    "        if oponent_sd['done']:\n",
    "            reward = 0\n",
    "            if oponent_sd['winner'] != '-':\n",
    "                # win or lose\n",
    "                reward += 10 if oponent_sd['winner'] == turn_mark else 0\n",
    "            # draw\n",
    "            reward += 0\n",
    "            \n",
    "            reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "            memory.push(obs, action, None, reward)\n",
    "        else:\n",
    "            if sd == oponent_sd:\n",
    "                #if random.random() > 0.7:\n",
    "                #    #ignore some invalid moves\n",
    "                #    continue\n",
    "                # invalid move\n",
    "                reward = -10\n",
    "                reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "                memory.push(obs, action, None, reward)\n",
    "            else:\n",
    "                for j in(range(n_actions)):\n",
    "                    \n",
    "                    # next_states for each oponent action based on the \"player\" action.\n",
    "                    next_sd = states_dict[oponent_sd['actions'][j]]\n",
    "                    if next_sd == oponent_sd:\n",
    "                        # invalid oponent action\n",
    "                        continue\n",
    "                    next_obs = torch.tensor(get_game_obs(next_sd), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "                    reward = 0\n",
    "                    #else:\n",
    "                    #   reward += next_sd['possible_wins']\n",
    "\n",
    "                    if next_sd['done'] and next_sd['winner'] != '-':\n",
    "                        reward += -10 if next_sd['winner'] != turn_mark else 0\n",
    "                        \n",
    "\n",
    "                    reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "\n",
    "                    # ('state', 'action','next_state', 'reward')\n",
    "                    memory.push(obs, action, next_obs, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71019"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transition(state=tensor([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 1.,  0.,  1.,  0.,  0.,  0., -1.,  0., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0.,  0., -1.,  0.,  0.,  0., -1.,  1.]], device='cuda:0'), action=tensor([[5]], device='cuda:0'), next_state=tensor([[ 1.,  0.,  0., -1., -1.,  1.,  0., -1.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  1.,  0., -1.,  0.,  0.,  0.,  0.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=tensor([[-1., -1.,  1.,  0., -1.,  0.,  1.,  0.,  0.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  1., -1.,  1., -1.,  0.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1., -1., -1.,  1., -1., -1.,  0.,  1.,  1.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=None, reward=tensor([10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1., -1.,  1.,  1.,  0., -1.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[7]], device='cuda:0'), next_state=tensor([[ 1., -1.,  1.,  1.,  0., -1., -1.,  1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]], device='cuda:0'), action=tensor([[8]], device='cuda:0'), next_state=tensor([[-1.,  0., -1.,  0.,  0.,  0.,  1.,  0.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 1., -1.,  0.,  0.,  0.,  0., -1.,  0.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0.,  0.,  0., -1.,  1.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[7]], device='cuda:0'), next_state=tensor([[ 1., -1.,  0.,  0., -1.,  1.,  0.,  1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  1.,  0., -1., -1.,  1., -1.,  0.,  1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 1.,  1., -1., -1., -1.,  1., -1.,  0.,  1.]], device='cuda:0'), reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=tensor([[ 1.,  0., -1., -1., -1.,  0.,  1.,  0.,  0.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  0.,  1., -1., -1.,  1., -1.,  1.]], device='cuda:0'), action=tensor([[5]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  0., -1.,  0.,  1.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=tensor([[ 0.,  1.,  0., -1.,  0.,  1.,  0., -1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  0.,  0.,  1., -1.,  0., -1.,  1.]], device='cuda:0'), action=tensor([[2]], device='cuda:0'), next_state=tensor([[ 0., -1.,  1.,  0.,  1., -1., -1., -1.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=tensor([[ 1.,  1.,  0., -1., -1.,  0.,  1., -1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1., -1.,  1.,  0.,  1.,  1.,  0., -1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=None, reward=tensor([10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1., -1., -1.,  0., -1.,  0.,  1.,  0.,  0.]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0.,  0., -1.,  1., -1.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=tensor([[ 1.,  0., -1., -1.,  1., -1.,  1.,  0., -1.]], device='cuda:0'), reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  1.,  1., -1., -1.,  0., -1.,  1.,  0.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1., -1.,  1., -1., -1.,  1.,  1.,  0.]], device='cuda:0'), action=tensor([[8]], device='cuda:0'), next_state=None, reward=tensor([10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  1., -1., -1.,  0.,  0., -1.,  0.,  0.]], device='cuda:0'), action=tensor([[3]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1., -1., -1., -1.,  1.,  0., -1.,  1.,  0.]], device='cuda:0'), action=tensor([[3]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1., -1.,  1.,  1.,  0., -1.,  0.,  0.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  1., -1., -1.,  0., -1.,  1.,  1.]], device='cuda:0'), action=tensor([[8]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[7]], device='cuda:0'), next_state=tensor([[ 0.,  0.,  0.,  0., -1.,  1., -1.,  1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  0.,  0.,  1.,  0., -1.,  1.,  0.]], device='cuda:0'), action=tensor([[2]], device='cuda:0'), next_state=tensor([[ 0., -1.,  1.,  0.,  1.,  0., -1.,  1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0.]], device='cuda:0'), action=tensor([[5]], device='cuda:0'), next_state=tensor([[ 1.,  0., -1.,  1., -1.,  1.,  0.,  0., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  0., -1.,  0., -1.,  0.,  1.,  1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 1.,  0., -1., -1.,  0., -1.,  0.,  1.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  1., -1.,  1.,  0.,  0.,  0.,  0., -1.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=tensor([[ 0.,  1., -1.,  1.,  1.,  0.,  0., -1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0., -1.,  0., -1., -1.,  0.,  0.,  1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  1., -1., -1., -1.,  0.,  1.,  0.,  1.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  0., -1., -1.,  1.,  0.,  0.,  1.,  0.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=tensor([[-1., -1., -1., -1.,  1.,  0.,  1.,  1.,  0.]], device='cuda:0'), reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  0., -1.,  1.,  0.,  0., -1.,  1.,  1.]], device='cuda:0'), action=tensor([[3]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1., -1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.]], device='cuda:0'), action=tensor([[2]], device='cuda:0'), next_state=tensor([[-1., -1.,  1.,  0.,  1., -1.,  0.,  1.,  0.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  0., -1.,  1., -1.,  1.,  0., -1., -1.]], device='cuda:0'), action=tensor([[8]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  1.,  0., -1.,  1., -1.,  0., -1.]], device='cuda:0'), action=tensor([[2]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  0., -1.,  1.,  0.,  0.,  0.,  0.]], device='cuda:0'), action=tensor([[7]], device='cuda:0'), next_state=tensor([[ 0., -1., -1., -1.,  1.,  0.,  0.,  1.,  0.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  1.]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=tensor([[-1.,  1.,  1., -1.,  0.,  0.,  0., -1.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  0., -1.,  1.,  0.,  0.,  1.,  0.,  0.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=tensor([[-1.,  0., -1.,  1.,  1.,  0.,  1., -1.,  0.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  1.,  0.,  0., -1.,  0., -1.,  1.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=tensor([[ 0., -1.,  1.,  0.,  1., -1., -1., -1.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=tensor([[-1.,  1., -1.,  0.,  0.,  0., -1.,  0.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.]], device='cuda:0'), action=tensor([[8]], device='cuda:0'), next_state=tensor([[ 0.,  0.,  0.,  0.,  0., -1.,  0., -1.,  1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0., -1.,  1.,  0.,  0.,  0.,  1., -1.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=tensor([[ 0., -1., -1.,  1.,  1.,  0.,  0.,  1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  1.,  0.,  0., -1.,  0.,  0., -1.,  0.]], device='cuda:0'), action=tensor([[3]], device='cuda:0'), next_state=tensor([[ 0.,  1., -1.,  1., -1.,  0.,  0., -1.,  0.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0., -1.,  1.,  1.,  1.,  0., -1.,  0., -1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 1., -1.,  1.,  1.,  1.,  0., -1., -1., -1.]], device='cuda:0'), reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 1.,  1.,  0., -1., -1.,  1., -1.,  1., -1.]], device='cuda:0'), action=tensor([[6]], device='cuda:0'), next_state=None, reward=tensor([-10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  0., -1.,  0.,  1.,  0., -1.,  0.]], device='cuda:0'), action=tensor([[1]], device='cuda:0'), next_state=tensor([[ 0.,  1.,  0., -1.,  0.,  1.,  0., -1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0., -1.,  0.,  0.,  1.,  0.,  1., -1.]], device='cuda:0'), action=tensor([[0]], device='cuda:0'), next_state=tensor([[ 1.,  0., -1., -1.,  0.,  1.,  0.,  1., -1.]], device='cuda:0'), reward=tensor([0.], device='cuda:0')),\n",
       " Transition(state=tensor([[-1.,  1.,  0., -1.,  1., -1.,  1., -1.,  1.]], device='cuda:0'), action=tensor([[2]], device='cuda:0'), next_state=None, reward=tensor([10.], device='cuda:0')),\n",
       " Transition(state=tensor([[ 0.,  0.,  1., -1.,  0., -1.,  1., -1.,  0.]], device='cuda:0'), action=tensor([[4]], device='cuda:0'), next_state=None, reward=tensor([10.], device='cuda:0'))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 71019 // 500 #343224 // 100 #1024\n",
    "GAMMA = 0.95\n",
    "TAU = 0.00005\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR) # amsgrad? r:\n",
    "global_step = 0\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # print after\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch) # print after\n",
    "    state_action_values = state_action_values.gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    expected_next_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_next_action_values.unsqueeze(1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  max_epoch = 20000\n",
    "else:\n",
    "  max_epoch = 50\n",
    "\n",
    "h_params = {\n",
    "    'REPLAY_SIZE': REPLAY_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'TAU': TAU,\n",
    "    'LR': LR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "ep_losses = []\n",
    "\n",
    "with SummaryWriter(log_dir=f'duel_runs/{timestr}') as writer:\n",
    "    \n",
    "    while global_step < max_epoch:\n",
    "\n",
    "\n",
    "        loss_scalar = optimize_model()\n",
    "        ep_losses.append(loss_scalar)\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "\n",
    "        loss_mean = np.mean(ep_losses)\n",
    "        \n",
    "        writer.add_hparams(\n",
    "            h_params,\n",
    "            {\n",
    "                'i_episode': global_step,\n",
    "                'Memory_len': len(memory),\n",
    "                'Loss': loss_mean, # loss_scalar,\n",
    "            }, name='.', global_step=global_step,\n",
    "        )\n",
    "        ep_losses = []\n",
    "        \n",
    "        global_step += 1 \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_model_dir = './duel_saved_models'\n",
    "\n",
    "if not os.path.exists(f'{save_model_dir}'):\n",
    "    os.mkdir(f'{save_model_dir}')\n",
    "if not os.path.exists(f'{save_model_dir}/{timestr}'):\n",
    "    os.mkdir(f'{save_model_dir}/{timestr}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), f'{save_model_dir}/{timestr}/policy_net')\n",
    "torch.save(target_net.state_dict(), f'{save_model_dir}/{timestr}/target_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\\n\\nmodel = DQN(n_observations, n_actions).to(device)\\nload_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\\nmodel.load_state_dict(load_dict)\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "load_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\n",
    "model.load_state_dict(load_dict)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-']\n",
      "['-', 'x', '-']\n",
      "['-', '-', '-']\n",
      "{'id': 4763, 'encoded': '----x----', 'actions': [4764, 4915, 5015, 5076, 4763, 5111, 5124, 5134, 5141], 'done': False, 'turn': 1, 'winner': '-', 'possible_wins': 0}\n",
      "['-', '-', '-']\n",
      "['-', 'x', '-']\n",
      "['-', '-', 'o']\n",
      "{'id': 5141, 'encoded': '----x---o', 'actions': [1867, 3254, 4163, 4759, 5141, 5142, 5143, 5144, 5141], 'done': False, 'turn': 2, 'winner': '-', 'possible_wins': 0}\n",
      "['-', 'x', '-']\n",
      "['-', 'x', '-']\n",
      "['-', '-', 'o']\n",
      "{'id': 3254, 'encoded': '-x--x---o', 'actions': [2385, 3254, 2687, 2909, 3254, 3171, 3217, 3242, 3254], 'done': False, 'turn': 3, 'winner': '-', 'possible_wins': 0}\n",
      "['-', 'x', '-']\n",
      "['-', 'x', '-']\n",
      "['-', 'o', 'o']\n",
      "{'id': 3242, 'encoded': '-x--x--oo', 'actions': [1839, 3242, 3233, 3238, 3242, 3243, 3244, 3242, 3242], 'done': False, 'turn': 4, 'winner': '-', 'possible_wins': 0}\n",
      "['-', 'x', '-']\n",
      "['-', 'x', '-']\n",
      "['x', 'o', 'o']\n",
      "{'id': 3244, 'encoded': '-x--x-xoo', 'actions': [2383, 3244, 2685, 2907, 3244, 3169, 3244, 3244, 3244], 'done': False, 'turn': 5, 'winner': '-', 'possible_wins': 0}\n",
      "['-', 'x', 'o']\n",
      "['-', 'x', '-']\n",
      "['x', 'o', 'o']\n",
      "{'id': 2685, 'encoded': '-xo-x-xoo', 'actions': [877, 2685, 2685, 2619, 2685, 2683, 2685, 2685, 2685], 'done': False, 'turn': 6, 'winner': '-', 'possible_wins': 0}\n",
      "['-', 'x', 'o']\n",
      "['-', 'x', 'x']\n",
      "['x', 'o', 'o']\n",
      "{'id': 2683, 'encoded': '-xo-xxxoo', 'actions': [2303, 2683, 2683, 2640, 2683, 2683, 2683, 2683, 2683], 'done': False, 'turn': 7, 'winner': '-', 'possible_wins': 0}\n",
      "['-', 'x', 'o']\n",
      "['o', 'x', 'x']\n",
      "['x', 'o', 'o']\n",
      "{'id': 2640, 'encoded': '-xooxxxoo', 'actions': [678, 2640, 2640, 2640, 2640, 2640, 2640, 2640, 2640], 'done': False, 'turn': 8, 'winner': '-', 'possible_wins': 0}\n",
      "['x', 'x', 'o']\n",
      "['o', 'x', 'x']\n",
      "['x', 'o', 'o']\n",
      "{'id': 678, 'encoded': 'xxooxxxoo', 'actions': [], 'done': True, 'turn': 9, 'winner': '-', 'possible_wins': 0}\n",
      "tensor([[ 0.,  1., -1., -1.,  1.,  1.,  1., -1., -1.]], device='cuda:0')\n",
      "['-', '-', '-']\n",
      "['-', 'x', '-']\n",
      "['-', '-', '-']\n",
      "{'id': 4763, 'encoded': '----x----', 'actions': [4764, 4915, 5015, 5076, 4763, 5111, 5124, 5134, 5141], 'done': False, 'turn': 1, 'winner': '-', 'possible_wins': 0}\n",
      "['-', '-', '-']\n",
      "['-', 'x', '-']\n",
      "['-', '-', 'o']\n",
      "{'id': 5141, 'encoded': '----x---o', 'actions': [1867, 3254, 4163, 4759, 5141, 5142, 5143, 5144, 5141], 'done': False, 'turn': 2, 'winner': '-', 'possible_wins': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/venvs/deep_learning/lib/python3.10/site-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/venvs/deep_learning/lib/python3.10/site-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#_, ax = plt.subplots(1, 1)\n",
    "\n",
    "#img = ax.imshow(env.render())\n",
    "\n",
    "\n",
    "while True:\n",
    "  # policy_net.eval()\n",
    "  state = states_dict[0]\n",
    "\n",
    "  for t in count():\n",
    "    obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      action = policy_net(obs)\n",
    "      action = action.max(1)[1].view(1,1)\n",
    "    \n",
    "    next_state = states_dict[state['actions'][action.item()]]\n",
    "    done = next_state['done']\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    for line in state_decode(state['encoded']):\n",
    "      print(line)\n",
    "    print(state)\n",
    "\n",
    "    if done:\n",
    "      print(obs)\n",
    "      input()\n",
    "      break\n",
    "    \n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#_, ax = plt.subplots(1, 1)\n",
    "\n",
    "#img = ax.imshow(env.render())\n",
    "\n",
    "\n",
    "while True:\n",
    "  # policy_net.eval()\n",
    "  state = states_dict[0]\n",
    "  model_turn = True if random.random() > 0.5 else False\n",
    "  for t in count():\n",
    "    clear_output()\n",
    "    if model_turn:\n",
    "      obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "      with torch.no_grad():\n",
    "        action = policy_net(obs)\n",
    "        action = action.max(1)[1].view(1,1)\n",
    "      \n",
    "      next_state = states_dict[state['actions'][action.item()]]\n",
    "      \n",
    "      \n",
    "    else:\n",
    "      for line in state_decode(state['encoded']):\n",
    "        print(line, flush = True)\n",
    "      action = int(input(\"player move: \"))\n",
    "      next_state = states_dict[state['actions'][action]]\n",
    "    done = next_state['done']\n",
    "    state = next_state\n",
    "    model_turn = not model_turn\n",
    "    if done:\n",
    "      for line in state_decode(state['encoded']):\n",
    "        print(line, flush = True)\n",
    "      input(\"press to continue...\")\n",
    "      break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
