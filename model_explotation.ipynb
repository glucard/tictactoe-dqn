{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from src.state_generator import generate_states\n",
    "\n",
    "if not os.path.exists(\"data/states.json\"):\n",
    "    generate_states()\n",
    "with open(\"data/states.json\") as file:\n",
    "    states_json = json.load(file)\n",
    "    states_dict = states_json['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action','next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    n = 1028\n",
    "    self.layer_input = nn.Linear(n_observations, n)\n",
    "    self.layer_h_1 = nn.Linear(n, n)\n",
    "    self.layer_h_2 = nn.Linear(n, n)\n",
    "    self.layer_v = nn.Linear(n, 1)\n",
    "    self.layer_a = nn.Linear(n, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_input(x))\n",
    "    # x = self.dropout0(x)\n",
    "    x = F.relu(self.layer_h_1(x))\n",
    "    # x = self.dropout1(x)\n",
    "    x = F.relu(self.layer_h_2(x))\n",
    "    # x = self.dropout2(x)\n",
    "    \n",
    "    v = self.layer_v(x)\n",
    "    a = self.layer_a(x)\n",
    "    \n",
    "    q = v + a - a.mean()\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "turn_marks = {\n",
    "    'x': {\n",
    "        'x': 1,\n",
    "        'o': -1,\n",
    "        '-': 0,\n",
    "    },\n",
    "    'o': {\n",
    "        'x': -1,\n",
    "        'o': 1,\n",
    "        '-': 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_game_obs(state_dict: dict) -> list:\n",
    "    \"\"\"\n",
    "    turn_mark = 'x' if state_dict['turn'] % 2 == 0 else 'o'\n",
    "    return [turn_marks[turn_mark][e] for e in state_dict['encoded']]\n",
    "    \"\"\"\n",
    "    x = [1 if e == 'x' else 0 for e in state_dict['encoded']]\n",
    "    o = [1 if e == 'o' else 0 for e in state_dict['encoded']]\n",
    "    turn_mark = [state_dict['turn'] % 2]\n",
    "\n",
    "    return x + o + turn_mark\n",
    "\n",
    "# all_games_obs = [get_game_obs(sd) for sd in states_dict]\n",
    "# get_game_obs(states_dict[2]), states_dict[2]\n",
    "sample_obs = get_game_obs(states_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "n_observations = len(sample_obs)\n",
    "n_actions = 9\n",
    "\n",
    "REPLAY_SIZE = len(states_dict) * n_actions**2\n",
    "\n",
    "memory = ReplayMemory(REPLAY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20 #343224 // 100 #1024\n",
    "GAMMA = 0.95\n",
    "TAU = 0.005\n",
    "LR = 0.0003\n",
    "EPS = 0.5\n",
    "EPS_DECAY = 0.9999\n",
    "EPS_MIN = 0.1\n",
    "\n",
    "epsilon = EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR) # amsgrad? r:\n",
    "global_step = 0\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # print after\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    expected_next_action_values = reward_batch + GAMMA * next_state_values\n",
    "    expected_next_action_values = expected_next_action_values.unsqueeze(1)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_next_action_values)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  max_epoch = 100_000\n",
    "else:\n",
    "  max_epoch = 50\n",
    "\n",
    "h_params = {\n",
    "    'REPLAY_SIZE': REPLAY_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'TAU': TAU,\n",
    "    'LR': LR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "WIN_REWARD = 1\n",
    "LOST_REWARD = -1\n",
    "DRAW_REWARD = -0.5\n",
    "INVALID_MOVE_REWARD = -3.0\n",
    "STEP_REWARD = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def tensor_reward(reward):\n",
    "    return torch.tensor([reward], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "with SummaryWriter(log_dir=f'duel_runs/{timestr}') as writer:\n",
    "    \n",
    "    while global_step < max_epoch:\n",
    "        \n",
    "        ep_losses = []\n",
    "        ep_rewards = []\n",
    "        ep_qvalues = []\n",
    "        ep_epsilon = []\n",
    "\n",
    "        state = states_dict[0]\n",
    "        obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        ai_turn = random.random() < 0.5\n",
    "        prev_obs = None\n",
    "        prev_action = None\n",
    "        for t in count():\n",
    "            ai_turn = not ai_turn\n",
    "            if ai_turn:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(obs)\n",
    "                    max_q_value = q_values.max(1)\n",
    "                    qvalue = max_q_value[0].item()\n",
    "                    action = max_q_value[1].view(1,1).item()\n",
    "                    \n",
    "                # epsilon = epsilon * EPS_DECAY\n",
    "                epsilon = EPS - (global_step / max_epoch) if epsilon > EPS_MIN else EPS_MIN\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.randint(0, 8)\n",
    "            else:\n",
    "                actions = state['actions']\n",
    "                state_id = state[\"id\"]\n",
    "                valid_actions = [a for a in actions if a != state_id]\n",
    "                action = actions.index(random.choice(valid_actions))\n",
    "\n",
    "            next_state = states_dict[state['actions'][action]]\n",
    "            next_obs = torch.tensor(get_game_obs(next_state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            done = next_state['done']\n",
    "            \n",
    "            turn_mark = 'x' if state ['turn'] % 2 == 0 else 'o'\n",
    "            \"\"\"\n",
    "            if done:\n",
    "                # print(\"done\")\n",
    "                if next_state['winner'] != '-':\n",
    "                    # print(\"win\")\n",
    "                    # win or lose\n",
    "                    if next_state['winner'] == turn_mark:\n",
    "                        reward = WIN_REWARD\n",
    "                else:\n",
    "                    reward = DRAW_REWARD\n",
    "                next_state = None\n",
    "            elif state == next_state:\n",
    "                next_state = None\n",
    "                reward = INVALID_MOVE_REWARD\n",
    "            \"\"\"\n",
    "            action = torch.tensor([[action]], dtype=torch.long).to(device)\n",
    "            #reward = tensor_reward(reward)\n",
    "\n",
    "            if state == next_state:\n",
    "                next_state = None\n",
    "                reward = INVALID_MOVE_REWARD\n",
    "                memory.push(obs, action, None, tensor_reward(reward))\n",
    "            elif done:\n",
    "                if next_state['winner'] != '-':\n",
    "                    if next_state['winner'] == turn_mark:\n",
    "                        reward = WIN_REWARD\n",
    "                        memory.push(obs, action, None, tensor_reward(reward))\n",
    "                        memory.push(prev_obs, prev_action, None, tensor_reward(LOST_REWARD))\n",
    "                    else:\n",
    "                        reward = LOST_REWARD\n",
    "                        memory.push(obs, action, None, tensor_reward(reward))\n",
    "                        memory.push(prev_obs, prev_action, None, tensor_reward(WIN_REWARD))\n",
    "                else:\n",
    "                    reward = DRAW_REWARD\n",
    "                    memory.push(obs, action, None, tensor_reward(reward))\n",
    "                    memory.push(prev_obs, prev_action, None, tensor_reward(reward))\n",
    "                next_state = None\n",
    "            else:\n",
    "                reward = STEP_REWARD\n",
    "                if prev_obs != None:\n",
    "                    memory.push(prev_obs, prev_action, next_obs, tensor_reward(reward))\n",
    "            \n",
    "            if ai_turn:\n",
    "                ep_rewards.append(reward)\n",
    "                ep_qvalues.append(qvalue)\n",
    "                ep_epsilon.append(epsilon)\n",
    "\n",
    "            if BATCH_SIZE < len(memory):\n",
    "                loss_scalar = optimize_model()\n",
    "\n",
    "                ep_losses.append(loss_scalar)\n",
    "\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "            if next_state == None:\n",
    "                break\n",
    "\n",
    "            # memory.push(obs, action, next_obs, reward)\n",
    "            \n",
    "            prev_action = action\n",
    "            prev_obs = obs\n",
    "\n",
    "            state = next_state\n",
    "            obs = next_obs\n",
    "            \n",
    "\n",
    "        if BATCH_SIZE < len(memory):\n",
    "            loss_mean = np.mean(ep_losses)\n",
    "            rewards_mean = np.mean(ep_rewards)\n",
    "            qvalues_mean = np.mean(ep_qvalues)\n",
    "            epsilons_mean = np.mean(ep_epsilon)\n",
    "            # reward_log = ep_rewards[-1] if ep_rewards != [] else 0\n",
    "            writer.add_hparams(\n",
    "                h_params,\n",
    "                {\n",
    "                    'i_episode': t,\n",
    "                    'Memory_len': len(memory),\n",
    "                    'Loss': loss_mean, # loss_scalar,\n",
    "                    'Reward': ep_rewards[-1],\n",
    "                    'Qvalue': qvalues_mean,\n",
    "                    'Epsilon': epsilons_mean\n",
    "                }, name='.', global_step=global_step,\n",
    "            )\n",
    "        \n",
    "        global_step += 1 \n",
    "        writer.flush()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_model_dir = './duel_saved_models'\n",
    "\n",
    "if not os.path.exists(f'{save_model_dir}'):\n",
    "    os.mkdir(f'{save_model_dir}')\n",
    "if not os.path.exists(f'{save_model_dir}/{timestr}'):\n",
    "    os.mkdir(f'{save_model_dir}/{timestr}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), f'{save_model_dir}/{timestr}/policy_net')\n",
    "torch.save(target_net.state_dict(), f'{save_model_dir}/{timestr}/target_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "load_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\n",
    "model.load_state_dict(load_dict)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#_, ax = plt.subplots(1, 1)\n",
    "\n",
    "#img = ax.imshow(env.render())\n",
    "\n",
    "from src.tictactoe import decode as state_decode\n",
    "\n",
    "while True:\n",
    "  # policy_net.eval()\n",
    "  state = states_dict[0]\n",
    "\n",
    "  for t in count():\n",
    "    obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      action = policy_net(obs).max(1)[1].view(1,1)\n",
    "    \n",
    "    next_state = states_dict[state['actions'][action.item()]]\n",
    "    done = next_state['done']\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    for line in state_decode(state['encoded']):\n",
    "      print(line, flush=True)\n",
    "    print(state, flush=True)\n",
    "\n",
    "    if done:\n",
    "      print(obs, flush=True)\n",
    "      input()\n",
    "      break\n",
    "    \n",
    "    input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
