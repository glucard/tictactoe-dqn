{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from src.state_generator import generate_states\n",
    "\n",
    "if not os.path.exists(\"data/states.json\"):\n",
    "    generate_states()\n",
    "with open(\"data/states.json\") as file:\n",
    "    states_json = json.load(file)\n",
    "    states_dict = states_json['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action','next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    n = 32\n",
    "    self.layer_input = nn.Linear(n_observations, n)\n",
    "    self.layer_h_1 = nn.Linear(n, n)\n",
    "    self.layer_h_2 = nn.Linear(n, n)\n",
    "    self.layer_v = nn.Linear(n, 1)\n",
    "    self.layer_a = nn.Linear(n, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_input(x))\n",
    "    # x = self.dropout0(x)\n",
    "    x = F.relu(self.layer_h_1(x))\n",
    "    # x = self.dropout1(x)\n",
    "    x = F.relu(self.layer_h_2(x))\n",
    "    # x = self.dropout2(x)\n",
    "    \n",
    "    v = self.layer_v(x)\n",
    "    a = self.layer_a(x)\n",
    "    \n",
    "    q = v + a - a.mean()\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_marks = {\n",
    "    'x': {\n",
    "        'x': 1,\n",
    "        'o': -1,\n",
    "        '-': 0,\n",
    "    },\n",
    "    'o': {\n",
    "        'x': -1,\n",
    "        'o': 1,\n",
    "        '-': 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_game_obs(state_dict: dict) -> list:\n",
    "    \"\"\"\n",
    "    turn_mark = 'x' if state_dict['turn'] % 2 == 0 else 'o'\n",
    "    return [turn_marks[turn_mark][e] for e in state_dict['encoded']]\n",
    "    \"\"\"\n",
    "    x = [1 if e == 'x' else 0 for e in state_dict['encoded']]\n",
    "    o = [1 if e == 'o' else 0 for e in state_dict['encoded']]\n",
    "    turn_mark = [state_dict['turn'] % 2]\n",
    "\n",
    "    return x + o + turn_mark\n",
    "\n",
    "# all_games_obs = [get_game_obs(sd) for sd in states_dict]\n",
    "# get_game_obs(states_dict[2]), states_dict[2]\n",
    "sample_obs = get_game_obs(states_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = len(sample_obs)\n",
    "n_actions = 9\n",
    "\n",
    "REPLAY_SIZE = len(states_dict) * n_actions**2\n",
    "\n",
    "memory = ReplayMemory(REPLAY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20 #343224 // 100 #1024\n",
    "GAMMA = 0.95\n",
    "TAU = 0.01\n",
    "LR = 0.00002\n",
    "EPS = 0.5\n",
    "EPS_DECAY = 0.9999\n",
    "EPS_MIN = 0.1\n",
    "\n",
    "epsilon = EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR) # amsgrad? r:\n",
    "global_step = 0\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # print after\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    expected_next_action_values = reward_batch + GAMMA * next_state_values\n",
    "    expected_next_action_values = expected_next_action_values.unsqueeze(1)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_next_action_values)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "DRAW_REWARD = 0.1\n",
    "INVALID_MOVE_REWARD = -1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  max_epoch = 10_000\n",
    "else:\n",
    "  max_epoch = 50\n",
    "\n",
    "h_params = {\n",
    "    'REPLAY_SIZE': REPLAY_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'TAU': TAU,\n",
    "    'LR': LR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "DRAW_REWARD = 0.1\n",
    "INVALID_MOVE_REWARD = -3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "with SummaryWriter(log_dir=f'duel_runs/{timestr}') as writer:\n",
    "    \n",
    "    while global_step < max_epoch:\n",
    "        \n",
    "        ep_losses = []\n",
    "        ep_rewards = []\n",
    "        ep_qvalues = []\n",
    "        ep_epsilon = []\n",
    "\n",
    "        state = states_dict[0]\n",
    "        obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(obs)\n",
    "                max_q_value = q_values.max(1)\n",
    "                qvalue = max_q_value[0].item()\n",
    "                action = max_q_value[1].view(1,1).item()\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, 8)\n",
    "            # epsilon = epsilon * EPS_DECAY\n",
    "            epsilon = EPS - (global_step / max_epoch) if epsilon > EPS_MIN else EPS_MIN\n",
    "\n",
    "            next_state = states_dict[state['actions'][action]]\n",
    "            done = next_state['done']\n",
    "            \n",
    "            turn_mark = 'x' if state ['turn'] % 2 == 0 else 'o'\n",
    "            reward = 0\n",
    "\n",
    "            if done:\n",
    "                # print(\"done\")\n",
    "                if next_state['winner'] != '-':\n",
    "                    # print(\"win\")\n",
    "                    # win or lose\n",
    "                    reward = WIN_REWARD if next_state['winner'] == turn_mark else 0\n",
    "                else:\n",
    "                    reward = DRAW_REWARD\n",
    "                next_state = None\n",
    "            elif state == next_state:\n",
    "                next_state = None\n",
    "                reward = INVALID_MOVE_REWARD\n",
    "\n",
    "            action = torch.tensor([[action]], dtype=torch.long).to(device)\n",
    "            reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "            \n",
    "            ep_rewards.append(reward.item())\n",
    "            ep_qvalues.append(qvalue)\n",
    "            ep_epsilon.append(epsilon)\n",
    "\n",
    "            if BATCH_SIZE < len(memory):\n",
    "                loss_scalar = optimize_model()\n",
    "\n",
    "                ep_losses.append(loss_scalar)\n",
    "\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "            if next_state == None:\n",
    "                memory.push(obs, action, None, reward)\n",
    "                break\n",
    "\n",
    "            next_obs = torch.tensor(get_game_obs(next_state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            memory.push(obs, action, next_obs, reward)\n",
    "            \n",
    "            state = next_state\n",
    "            obs = next_obs\n",
    "\n",
    "        if BATCH_SIZE < len(memory):\n",
    "            loss_mean = np.mean(ep_losses)\n",
    "            rewards_mean = np.mean(ep_rewards)\n",
    "            qvalues_mean = np.mean(ep_qvalues)\n",
    "            epsilons_mean = np.mean(ep_epsilon)\n",
    "\n",
    "            writer.add_hparams(\n",
    "                h_params,\n",
    "                {\n",
    "                    'i_episode': t,\n",
    "                    'Memory_len': len(memory),\n",
    "                    'Loss': loss_mean, # loss_scalar,\n",
    "                    'Reward': ep_rewards[-1],\n",
    "                    'Qvalue': qvalues_mean,\n",
    "                    'Epsilon': epsilons_mean\n",
    "                }, name='.', global_step=global_step,\n",
    "            )\n",
    "        \n",
    "        global_step += 1 \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_model_dir = './duel_saved_models'\n",
    "\n",
    "if not os.path.exists(f'{save_model_dir}'):\n",
    "    os.mkdir(f'{save_model_dir}')\n",
    "if not os.path.exists(f'{save_model_dir}/{timestr}'):\n",
    "    os.mkdir(f'{save_model_dir}/{timestr}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), f'{save_model_dir}/{timestr}/policy_net')\n",
    "torch.save(target_net.state_dict(), f'{save_model_dir}/{timestr}/target_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\\n\\nmodel = DQN(n_observations, n_actions).to(device)\\nload_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\\nmodel.load_state_dict(load_dict)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"_MODEL_DATE_NAME = '2024_04_25_12_56_11'\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "load_dict = torch.load(f'./duel_saved_models/{_MODEL_DATE_NAME}/policy_net')\n",
    "model.load_state_dict(load_dict)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', '-']\n",
      "['-', 'x', '-']\n",
      "['-', '-', '-']\n",
      "{'id': 4763, 'encoded': '----x----', 'actions': [4764, 4915, 5015, 5076, 4763, 5111, 5124, 5134, 5141], 'done': False, 'turn': 1, 'winner': '-', 'possible_wins': 0}\n",
      "['-', '-', '-']\n",
      "['-', 'x', '-']\n",
      "['o', '-', '-']\n",
      "{'id': 5124, 'encoded': '----x-o--', 'actions': [1817, 3212, 4128, 4731, 5124, 5125, 5124, 5130, 5132], 'done': False, 'turn': 2, 'winner': '-', 'possible_wins': 0}\n",
      "['-', '-', '-']\n",
      "['-', 'x', 'x']\n",
      "['o', '-', '-']\n",
      "{'id': 5125, 'encoded': '----xxo--', 'actions': [4832, 4952, 5030, 5078, 5125, 5125, 5125, 5126, 5128], 'done': False, 'turn': 3, 'winner': '-', 'possible_wins': 0}\n",
      "['-', '-', '-']\n",
      "['-', 'x', 'x']\n",
      "['o', 'o', '-']\n",
      "{'id': 5126, 'encoded': '----xxoo-', 'actions': [1819, 3214, 4130, 4733, 5126, 5126, 5126, 5126, 5127], 'done': False, 'turn': 4, 'winner': '-', 'possible_wins': 1}\n",
      "['-', '-', '-']\n",
      "['x', 'x', 'x']\n",
      "['o', 'o', '-']\n",
      "{'id': 4733, 'encoded': '---xxxoo-', 'actions': [], 'done': True, 'turn': 5, 'winner': 'x', 'possible_wins': 0}\n",
      "tensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     29\u001b[0m   \u001b[38;5;28mprint\u001b[39m(obs, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m   \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28minput\u001b[39m()\n",
      "File \u001b[0;32m~/tools/venvs/deep_learning/lib/python3.10/site-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/venvs/deep_learning/lib/python3.10/site-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#_, ax = plt.subplots(1, 1)\n",
    "\n",
    "#img = ax.imshow(env.render())\n",
    "\n",
    "from src.tictactoe import decode as state_decode\n",
    "\n",
    "while True:\n",
    "  # policy_net.eval()\n",
    "  state = states_dict[0]\n",
    "\n",
    "  for t in count():\n",
    "    obs = torch.tensor(get_game_obs(state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      action = policy_net(obs).max(1)[1].view(1,1)\n",
    "    \n",
    "    next_state = states_dict[state['actions'][action.item()]]\n",
    "    done = next_state['done']\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    for line in state_decode(state['encoded']):\n",
    "      print(line, flush=True)\n",
    "    print(state, flush=True)\n",
    "\n",
    "    if done:\n",
    "      print(obs, flush=True)\n",
    "      input()\n",
    "      break\n",
    "    \n",
    "    input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
